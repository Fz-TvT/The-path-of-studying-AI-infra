# FlashAttention åŸç†
## ç®€ä»‹å’Œç›¸å…³å‰ç½®

FlashAttention çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡èåˆ(fusing)æ³¨æ„åŠ›è®¡ç®—ä¸­çš„å¤šä¸ªæ“ä½œ(å¦‚ Q, K, V çŸ©é˜µä¹˜æ³•ã€softmax å½’ä¸€åŒ–ã€ä¸ V çš„åŠ æƒæ±‚å’Œç­‰)ï¼Œç›´æ¥åœ¨ç¡¬ä»¶(å¦‚ GPU)çš„é«˜é€Ÿç¼“å­˜ï¼ˆSRAMï¼‰ä¸­å®Œæˆè®¡ç®—ï¼Œä»è€Œæœ€å°åŒ–å¯¹æ…¢é€Ÿå…¨å±€å†…å­˜(å¦‚ HBM)çš„è¯»å†™æ¬¡æ•°ã€‚è¿™è¢«ç§°ä¸ºâ€œIO æ„ŸçŸ¥â€(IO-aware)ã€‚ å…·ä½“è€Œè¨€ï¼ŒFlashAttentionä½¿ç”¨==å¹³é“ºå’Œé‡è®¡ç®—==ç­‰ç»å…¸æŠ€æœ¯ï¼Œå°†è¾“å…¥å—ä»HBMåŠ è½½åˆ°SRAM(å¿«é€Ÿç¼“å­˜)ï¼Œåœ¨SRAMä¸Šæ‰§è¡Œæ³¨æ„åŠ›æ“ä½œï¼Œå¹¶å°†ç»“æœæ›´æ–°å›HBMã€‚FlashAttentionå‡å°‘äº†å†…å­˜è¯»å†™é‡ï¼Œä»è€Œå®ç°äº†2-4å€çš„æ—¶é’Ÿæ—¶é—´åŠ é€Ÿã€‚

------
-  HBM(high bandwidth memory) å’Œ SRAMï¼ˆstatic random-access memoryï¼‰ã€‚é€Ÿåº¦ä¸ŠSRAM>HBM>DRAM
-  MAC(Memory Access Cost) æ˜¯æŒ‡åœ¨è®¡ç®—æœºç³»ç»Ÿä¸­ï¼Œè®¿é—®å†…å­˜æˆ–å­˜å‚¨å™¨æ‰€éœ€çš„æ—¶é—´å’Œèµ„æºå¼€é”€ã€‚å®ƒæ˜¯è¡¡é‡è®¡ç®—æœºç¨‹åºæˆ–ç®—æ³•æ€§èƒ½çš„é‡è¦æŒ‡æ ‡ä¹‹ä¸€ã€‚ MACçš„å€¼å–å†³äºå¤šä¸ªå› ç´ ï¼ŒåŒ…æ‹¬å†…å­˜å±‚æ¬¡ç»“æ„ã€ç¼“å­˜å‘½ä¸­ç‡ã€å†…å­˜å¸¦å®½ã€å­˜å‚¨å™¨å»¶è¿Ÿç­‰ã€‚è¾ƒä½çš„MACå€¼è¡¨ç¤ºè®¿é—®å†…å­˜çš„å¼€é”€è¾ƒå°ï¼Œè€Œè¾ƒé«˜çš„MACå€¼è¡¨ç¤ºè®¿é—®å†…å­˜çš„å¼€é”€è¾ƒå¤§ã€‚

## åŸç†

### ä¼ ç»ŸAttention
å¯¹äºè¾“å…¥åºåˆ— $Q,K,V\in R^{N\times d}$ï¼Œ$N$æ˜¯åºåˆ—é•¿åº¦ï¼Œ$d$æ˜¯tokenå°ºå¯¸ï¼Œ$N>>d$ã€‚

self-attentionè®¡ç®—è¾“å‡º $O \in R^{Nd}$ , è®¡ç®—å…¬å¼ä¸ºï¼š
$$
S=QK^T \in R^{N\times d}ï¼ŒP=softmax(S)\in R^{N\times N},O=PV\in R^{N\times d}
$$
ä¼ ç»Ÿattentionè®¡ç®—æµç¨‹ä¸º:<!-- åŒçº§ç›®å½• -->
![è®¡ç®—æµç¨‹attention](picture/image.png)

### FlashAttention
æ ¸å¿ƒæ€æƒ³æ˜¯ä¼ ç»Ÿå‡å°‘HBMçš„è®¿é—®ï¼Œå°†$QKV$åˆ‡åˆ†ä¸ºå°å—åæ”¾å…¥SRAMä¸­
æ ¸å¿ƒæ–¹æ³•æ˜¯tilingå’Œrecomputation
#### Tiling(å¹³é“º):åˆ†å—è®¡ç®—
Softmaxè®¡ç®—æ–¹æ³•ï¼š
$$softmax(x_j)=\frac{e^{x_j}}{\sum_{i=1}^k{e^{x_i}}}$$
softmaxæ“ä½œæ˜¯row-wiseçš„ï¼Œå³æ¯è¡Œéƒ½ç®—ä¸€æ¬¡softmaxï¼Œæ‰€ä»¥éœ€è¦ç”¨åˆ°å¹³é“ºç®—æ³•æ¥åˆ†å—è®¡ç®—softmaxã€‚
åŸå§‹softmaxæ•°å€¼ä¸ç¨³å®šï¼Œä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼ŒFlashAttentioné‡‡ç”¨safe softmaxï¼Œå‘é‡ $\in R$ çš„safe softmax è®¡ç®—å¦‚ä¸‹

$$m(x): = \max _{i} x_{i}, \quad f(x): = \left[\begin{array}{lll}
e^{x_{1}-m(x)} & \ldots & e^{x_{B}-m(x)}
\end{array}\right], \quad \ell(x): = \sum_{i} f(x)_{i} \\
\operatorname{softmax}(x): = \frac{f(x)}{\ell(x)}$$
åŒç†ï¼Œåˆ™$x=[x^{(1)}x^{(2)}] \in R^{2B}$ çš„softmaxä¹Ÿå¯ä»¥é€šè¿‡åˆ†è§£è¿›è¡Œè®¡ç®—ï¼š
$$\begin{aligned}
m(x) & =m\left(\left[x^{(1)} x^{(2)}\right]\right)=\max \left(m\left(x^{(1)}\right), m\left(x^{(2)}\right)\right), \\
f(x) & =\left[e^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) \quad e^{m\left(x^{(2)}\right)-m(x)} f\left(x^{(2)}\right)\right], \\
\ell(x) & =\ell\left(\left[x^{(1)} x^{(2)}\right]\right)=e^{m\left(x^{(1)}\right)-m(x)} \ell\left(x^{(1)}\right)+e^{m\left(x^{(2)}\right)-m(x)} \ell\left(x^{(2)}\right), \\
\operatorname{softmax}(x) & =\frac{f(x)}{\ell(x)}
\end{aligned}$$

$f(x)$å’Œ$l(x)$éƒ½å¯ä»¥é€šè¿‡åˆ†å—è®¡ç®—å¾—å‡ºï¼Œæ‰€ä»¥FlashAttentionåœ¨è®¡ç®—æ—¶é€šè¿‡åˆ†å—å°†$Qï¼ŒKï¼ŒV$åˆ†å—åï¼ŒæŒ‰å—åŠ è½½åˆ°å†…å­˜ä¸­ã€‚

#### å‰å‘è®¡ç®—æ­¥éª¤
**ä¼ªä»£ç ï¼š**

![è®¡ç®—æµç¨‹attention](picture/forward.png)

**Algorithm 1 returns $O=softmax(QK^T)V$ with $ğ‘‚(N^2d)$ FLOPs and requires $O(b)$ additional memory beyond inputs and output.**

<details>
<summary>å‰å‘è®¡ç®—Pythonä»£ç </summary>

```python
import torch
NEG_INF = -1e10  # -infinity
EPSILON = 1e-10
Q_LEN = 6
K_LEN = 6
Q_BLOCK_SIZE = 3
KV_BLOCK_SIZE = 3
P_DROP = 0.2
Tr = Q_LEN // Q_BLOCK_SIZE
Tc = K_LEN // KV_BLOCK_SIZE
Q = torch.randn(1, 1, Q_LEN, 4, requires_grad=True).to(device='cpu')
K = torch.randn(1, 1, K_LEN, 4, requires_grad=True).to(device='cpu')
V = torch.randn(1, 1, K_LEN, 4, requires_grad=True).to(device='cpu')
O = torch.zeros_like(Q, requires_grad=True)
l = torch.zeros(Q.shape[:-1])[..., None]
m = torch.ones(Q.shape[:-1])[..., None] * NEG_INF
//step 4
Q_BLOCKS = torch.split(Q, Q_BLOCK_SIZE, dim=2)
K_BLOCKS = torch.split(K, KV_BLOCK_SIZE, dim=2)
V_BLOCKS = torch.split(V, KV_BLOCK_SIZE, dim=2)
//step 5
O_BLOCKS = list(torch.split(O, Q_BLOCK_SIZE, dim=2))
l_BLOCKS = list(torch.split(l, Q_BLOCK_SIZE, dim=2))
m_BLOCKS = list(torch.split(m, Q_BLOCK_SIZE, dim=2))
//step 6
for j in range(Tc):
    //step 7
    Kj = K_BLOCKS[j]
    Vj = V_BLOCKS[j]
    //step 8
    for i in range(Tr):
        //step 9
        Qi = Q_BLOCKS[i]
        Oi = O_BLOCKS[i]
        li = l_BLOCKS[i]
        mi = m_BLOCKS[i]
        //step 10
        S_ij = torch.einsum('... i d, ... j d -> ... i j', Qi, Kj)
        //step 11
        mask = S_ij.ge(0.5)
        S_ij = torch.masked_fill(S_ij, mask, value=0)
        //step 12
        m_block_ij, _ = torch.max(S_ij, dim=-1, keepdims=True)
        P_ij = torch.exp(S_ij - m_block_ij)
        l_block_ij = torch.sum(P_ij, dim=-1, keepdims=True) + EPSILON
        P_ij_Vj = torch.einsum('... i j, ... j d -> ... i d', P_ij, Vj)
        //step 13
        mi_new = torch.maximum(m_block_ij, mi)
        li_new = torch.exp(mi - mi_new) * li + \
                torch.exp(m_block_ij - mi_new) * l_block_ij
        //step 14
        m = torch.nn.Dropout(p=P_DROP)
        P_ij_Vj = m(P_ij_Vj)
        // Step 15
        O_BLOCKS[i] = (li / li_new) * torch.exp(mi - mi_new) * Oi \
                    + (torch.exp(m_block_ij - mi_new) / li_new) * P_ij_Vj
        print(f'-----------Attention : Q{i}xK{j}---------')
        print(O_BLOCKS[i].shape)
        print(O_BLOCKS[0])
        print(O_BLOCKS[1])
        print('\n')
        // step 16
        l_BLOCKS[i] = li_new
        m_BLOCKS[i] = mi_new

O = torch.cat(O_BLOCKS, dim=2)
l = torch.cat(l_BLOCKS, dim=2)
m = torch.cat(m_BLOCKS, dim=2)